{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Spark Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Spark Cluster, Spark Execution\n",
    "\n",
    "Spark cluster parable of classroom of teacher & students\n",
    "- each table of students is like an executor & each student is a \"core\"\n",
    "    - Each \"core\" is given an individual task.\n",
    "\n",
    "Scenario 2: count total pieces in \"candy bags\"\n",
    "- stage1: local count, each partition gets distributed to a core on a different executor.\n",
    "    - driver takes result of which ever executor finishes first, then the rest of the executors commit their results after\n",
    "    - Then stage1 is complete\n",
    "- stage2: global count, another executor fetches all the counts from each executor after all the counts complete. These results get passed to the driver, stage2 is then complete.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "\n",
    "### Shuffling & Caching\n",
    "\n",
    "groupBy triggers a _Wide operation_. Other wide transformations include:\n",
    "- `distinct, sort, join`\n",
    "\n",
    "_Narrow transformations_ include:\n",
    "- `select, filter, cast, union`\n",
    "\n",
    "Narrow transformations = when the data is required to compute the recs in a single partition that all reside in at most 1 partition of the parent rdd.\n",
    "\n",
    "Wide transformations = when the data is required to compute the records in a single partition that may reside in many partitions of the parent rdd.\n",
    "\n",
    "A shuffle introduces(i.e. demarcates) stage boundaries, shuffles happen when a wide transformation happens.\n",
    "Each shuffle requires a shuffle read (from disk), and a shuffle write (to disk).\n",
    "1st, shuffle writes write to disk so that all subsequent shuffle reads can read those files. Shuffle writes only happen once.\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Query Optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Query Optimization\n",
    "\n",
    "We can turn on the Adaptive Query Execution to improve the logical plan and physical plan.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Demo notes\n",
    "\n",
    "Shown an example of how a cache could accidentally block a predicate pushdown.\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "--------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Transcript\n",
    "0sec:\n",
    "Let's discuss partitioning. In this lesson you will understand the relationship between partitions and slots & cores.\n",
    "You'll configure default shuffle partitions, describe repartition & coalesce,\n",
    "match the number of partitions to the number of slots & cores,\n",
    "and you'll describe dynamic coalescing of shuffle partitions in AQE.\n",
    "\n",
    "28sec:\n",
    "The spark api uses the term \"core\", meaning a thread available for parallel execution.\n",
    "\n",
    "Here we refer to it as a slot to avoid confusion with the number of cores in the underlying CPU.\n",
    "To which there isn't necessarily an equal number.\n",
    "\n",
    "In most cases, if you created a cluster, you should know how many cores you have.\n",
    "However, to check programmatically, you can use:\n",
    "**`spark.sparkContext.defaultParallelism`**\n",
    "\n",
    "1m2s, Cores in Cluster:\n",
    "For operations like parallelize with no parent RDDs, it depends on the cluster manager.\n",
    "In local mode, you'll have a number of cores on the local machine.\n",
    "Mesos fine grain mode 8 and others have a total number of cores on all executor nodes or 2,\n",
    "whichever is larger.\n",
    "\n",
    "1m21s, Partitions of Data:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "//"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Partitioning\n",
    "\n",
    "Understand the relationship between partitions and slots & cores\n",
    "configure default shuffle partitions\n",
    "describe repartition & coalesce\n",
    "match number of partitions to slots & cores\n",
    "describe dynamic coalescing of shuffle partitions in AQE\n",
    "\n",
    "In Spark, a \"core\" is a thread available for parallel execution.\n",
    "We refer to them interchangably as slots to avoid confusion w/cores from the CPU.\n",
    "`spark.sparkContext.defaultParallelism`\n",
    "\n",
    "To get the number of partitions\n",
    "`df.rdd.getNumPartitions()`\n",
    "\n",
    "Let's say we have a pq file that was saved in 5 partitions with 8 available slots\n",
    "We have 2 options to *repartition*: .coalesce(N) or .repartition(N)\n",
    "pro's & con's: shuffle vs non-shuffle, even re-distribution, decrease-only vs increase-decrease\n",
    "\n",
    "We generally want the number of partitions to a multiple of the number of available slots\n",
    "e.g. if we have 4 slots ideally we should have 8 or 12 or 16... partitions\n",
    "\n",
    "A **very general guideline** is to have each partition be roughly about 200MB... ballpark\n",
    "\n",
    "On an executor with a reduced amount of RAM, we might need to lower the 200MB estimate. \n",
    "e.g. at 8 partitions corresponding to 4 slots, we would use close to 1.5GB \n",
    "If there are a lot of transformations that balloon each partition size, we will have problems.\n",
    "So when there are a lot of transformations that balloon each partition size making the initial \n",
    "partition size, say 50MB, would probably be better. \n",
    "\n",
    "6m30s, Matching the number of partitions to slots\n",
    "If there are 8 slots and 10 partitions, should we increase to 16 partitions or decrease to 8 partitions?\n",
    "... Answer: \n",
    "\n",
    "\n",
    "10m45s, Adaptive Queary Execution\n",
    "\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Demo notes\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Lab notes\n",
    "\n",
    "\n",
    "//"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

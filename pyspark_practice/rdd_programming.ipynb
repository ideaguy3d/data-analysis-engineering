{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# RDD Programming\n",
    "https://spark.apache.org/docs/3.0.2/rdd-programming-guide.html\n",
    "\n",
    "- Overview ✅\n",
    "- Linking with Spark ✅\n",
    "- Initializing Spark ✅\n",
    "    - Using the Shell ✅\n",
    "- Resilient Distributed Datasets (RDDs) ✅\n",
    "    - Parallelized Collections ✅\n",
    "    - External Datasets ✅ (_mostly a waste of time_)\n",
    "    - RDD Operations ✅ (_pretty good_)\n",
    "        - Basics ✅ (_really good_)\n",
    "        - Passing Functions to Spark ✅ (_kind of sucked_)\n",
    "        - Understanding closures ✅\n",
    "            - Example ✅\n",
    "            - Local vs. cluster modes ✅ (_REALLY REALLY important to understand_)\n",
    "            - Printing elements of an RDD ✅ (_good to know_)\n",
    "        - Working with Key-Value Pairs ✅ (_good_)\n",
    "        - Transformations\n",
    "        - Actions\n",
    "        - Shuffle operations\n",
    "            - Background\n",
    "            - Performance Impact\n",
    "    - RDD Persistence\n",
    "        - Which Storage Level to Choose?\n",
    "        - Removing Data\n",
    "    - Shared Variables\n",
    "    - Broadcast Variables\n",
    "    - Accumulators\n",
    "- Deploying to a Cluster\n",
    "- Launching Spark jobs from Java / Scala\n",
    "- Unit Testing\n",
    "- Where to Go from Here\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "\n",
    "spark = SparkSession.builder.appName(\"RDD_Programming\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "data = [1, 2, 3, 4, 5]\n",
    "dist_data = spark.sparkContext.parallelize(data)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "data": {
      "text/plain": "ParallelCollectionRDD[0] at readRDDFromFile at PythonRDD.scala:274"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "data": {
      "text/plain": "data/mango.txt MapPartitionsRDD[2] at textFile at NativeMethodAccessorImpl.java:0"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "All of Spark’s file-based input methods, including textFile, support running on directories, compressed files, and wildcards as well.\n",
    "For example, you can use textFile(\"/my/directory\"), textFile(\"/my/directory/*.txt\"), and textFile(\"/my/directory/*.gz\").\n",
    "'''\n",
    "dist_file_data = spark.sparkContext.textFile(\"data/mango.txt\")\n",
    "dist_file_data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## External Datasets\n",
    "\n",
    "mentions methods:\n",
    "- .wholeTextFiles(path)\n",
    "- .textFile(path)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "data": {
      "text/plain": "['this mango company animal',\n 'cat dog ant mic laptop mango',\n 'chair switch mobile am charger cover',\n 'amanda mango mango any alarm ant']"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_file_data_list = dist_file_data.collect()\n",
    "dist_file_data_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "[25, 28, 36, 32]"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dist_file_data = dist_file_data.map(lambda l: len(l))\n",
    "dist_file_data.collect()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "121"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# can optionally run:\n",
    "# dist_file_data.persist() # before the action to cache the data in memory across the cluster\n",
    "total_len = dist_file_data.reduce(lambda a, b: a + b)\n",
    "total_len"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "['Apple', 'Mic', 'Mic', 'Apple', 'Laptop', 'Mic']"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[('Apple', 1), ('Mic', 1), ('Mic', 1), ('Apple', 1), ('Laptop', 1), ('Mic', 1)]"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "[('Apple', 2), ('Mic', 3), ('Laptop', 1)]"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# from the \"Working with Key-Value Pairs\" section\n",
    "lines = spark.sparkContext.textFile(\"data/words_sm.txt\")\n",
    "display(lines.collect())\n",
    "pairs = lines.map(lambda s: (s, 1))\n",
    "display(pairs.collect())\n",
    "counts = pairs.reduceByKey(lambda a, b: a + b)\n",
    "display(counts.collect())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
